%% this chapter sets up the foundational system, which is Univalent Foundations
\label{ch:univalent-mathematics}

\section{What is a type?}
\label{sec:what-is-a-type}

In some computer programming languages, all variables are introduced along with a declaration of the type of thing they will refer to.  For example,
one may encounter types such as $bool$, $string$, $int$, and $real$, describing Boolean values, character strings, 32 bit integers, and 64 bit
floating point numbers.  The types are used to determine which statements of the programming language are grammatically well-formed.
For example, if $s$ of type $string$ and $x$ is of type $real$, we may write $1/x$, but we may not write $1/s$.

Types occur in mathematics, too, and are used in the same way: all variables are introduced along with a declaration of the type of thing they
will refer to.  For example, one may say ``consider a point $P$ of the plane'', ``consider a line $L$ of the plane'', ``consider a hexagon
$H$'', or ``consider a graph $G$''.  The types are used to determine which mathematical statements are grammatically well-formed.  For example,
one may write ``$P$ lies on $L$'' or ``$L$ passes through $P$'', but not ``$L$ lies on $P$''.

In \emph{univalent mathematics}, types are used to classify all mathematical objects.  Every mathematical object is an element of some (unique)
type.

One expresses the statement that an ``element'' $a$ is of ``type'' $X$ by writing $a:X$.  Using that notation, each variable is introduced along
with a declaration of the type of thing it will refer to, and the declared types of the variables are used to determine which statements of the
theory are grammatically well-formed.

There are enough ways to form new types from old ones to provide everything we need to write mathematics.

If $X$ and $Y$ are types, there will be a type whose elements serve as \emph{functions} from $X$ to $Y$; the notation for it is $X \to Y$.  Thus
when we write $f : X \to Y$, we mean that $f$ is an element of the type $X \to Y$, and we are saying that $f$ is a function from $X$ to $Y$.

Functions behave as one would expect, and one can make new ones in the usual way.

To provide an example of making new functions in the usual way, consider functions $f : X \to Y$ and $g : Y \to Z$.  We define their composite
$g \circ f : X \to Z$ by setting $g \circ f \defeq (a \mapsto g(f(a)))$.  Such definitions are to be regarded as syntactically transparent in
our formal system, in the sense that two formal expressions will be regarded as being \emph{the same by definition} if they yield the same formal
expression after the definitions of all the symbols within them are completely expanded.  Given two expressions that are the same by definition,
we may replace one with the other in any other expression, at will.  Here is an example: consider functions $f : X \to Y$, $g : Y \to Z$, and $h
: Z \to W$.  Then $(h \circ g) \circ f$ and $h \circ (g \circ f)$ are the same by definition, since applying the definitions within expands both
to $a \mapsto h(g(f(a)))$.

One may define the identity function $id_X : X \to X$ by setting $id_X \defeq (a \mapsto a)$.  Application of definitions shows that $f \circ
id_X$ is the same as $a \mapsto f(a)$, which, by convention, is to be regarded as the same as $f$.  A similar computation applies to $id_Y \circ
f$.

In the following sections we will expose various other elementary types and elementary ways to make new types from old ones.

\section{The type of natural numbers}
\label{sec:natural-numbers}

Here are Peano's rules \citep{peano-principia} for constructing the natural numbers in the form that is used in type theory.
\begin{enumerate}
\item[P1:] there is a type called $\NN$ (whose elements will be called natural numbers);
\item[P2:] there is an element of $\NN$ called $0$;
\item[P3:] if $m$ is a natural number, then there is also a natural number $S(m)$, called the \emph{successor} of $m$;
\item[P4:] given a family of types $X(m)$ depending on a parameter
  $m$ of type $\NN$, in order to define a family $f(m) : X(m)$ of elements of each of them it suffices to provide an element $a$ of $X(0)$ and
  to provide, for each $m$, a function $g_m : X(m) \to X(S(m))$.  (The resulting function $f$ may be regarded as having been defined inductively
  by the two declarations $f(0) \defeq a$ and $f(S(m)) \defeq g_m(f(m))$.)
\end{enumerate}
\nopagebreak
You may recognize rule P4 as ``the principle of mathematical induction'' or as ``defining a function by recursion''.  We may also refer to it
simply as ``induction for $\NN$''.  Notice that the two cases in an inductive definition correspond to the two ways of introducing elements of
$\NN$ via the use of rules P2 and P3.

We introduce the following syntactic definitions.
\begin{align*}
 1 & \defeq S(0) \\
 2 & \defeq S(1) \\
 t+1 & \defeq S(t),\text{~for any term $t$ of type $\NN$}
% 3 & \defeq S(2) \\
% 4 & \defeq S(3)
\end{align*}

Here is an example of defining a function by recursion using induction for $\NN$.
Assuming that we already have defined multiplication
(see \cref{sec:more-on-N}), we can define the factorial 
function $\fact : \NN \to \NN$ by defining $X(m)$ to be $\NN$ for all $m$ and 
by setting $\fact(0) \defeq 1$ and setting $\fact(m+1) \defeq (m+1) \cdot \fact(m)$.
The latter definition applies rule P4 with $1$ for $a$,
and the function $n \mapsto (m+1) \cdot n$ for $g_m$, for all $m$. 
Application of definition shows, for example,
that $\fact(2)$ and $2$ are the same by definition,
because they both reduce to $S(S(0))$.

Before we can write an equation such as $\fact(2)=2$, 
we must introduce a formal treatment of equality in type theory.  
We do that in the next section.
We finish this section with an example of iteration,
which is a special case of recursion. 

\begin{definition}\label{def:n-fold-iteration}
Let $X$ be a type and $e$ a function from $X$ to $X$. 
We define by induction the $n$-fold \emph{iteration} of $e$,
denoted $e^n$, and of the same type as $e$,
by setting $e^0 \defeq \id_X$ and $e^{n+1}\defeq e\circ e^n$.
\end{definition}

The reader may wonder how this definition, 
which should be clear enough, relates to rule P4.
We spell this out in full only once, and will take
the more liberal approach from \cref{def:n-fold-iteration}
throughout this book.
Define $X(m)$ to be $X\to X$ for all $m$ in $\NN$.
Apply rule P4 with the identity function $\id_X$ for $a$
and the function $d \mapsto e\circ d$ for each
$g_m : (X\to X)\to(X\to X)$. (Note that $g_m$ does not depend on $m$, 
which is typical for iteration, but not for recursion.) 
By rule P4 this defines a function $f: \NN\to(X\to X)$. 
Now define $e^n \defeq f(n)$ and we get $e^n$ as in
\cref{def:n-fold-iteration}.

%We may use induction to define the sum $n+m$ of two natural numbers, as a natural number.  We handle the two possible cases for the argument $m$
%as follows: we define $n+0 \defeq n$, and we define $n+S(m) \defeq S(n+m)$.  Application of definitions shows, for example, that $2+2$ and $4$
%are the same by definition, because they both reduce to $S(S(S(S(0))))$.

\section{Identity types}
\label{sec:identity-types}

One of the most important types is the \emph{identity type}, which implements the intuitive notion of equality; the reader may be more
comfortable if we call it the \emph{equality type}, at least initially.  Identity (or equality) between two elements may be considered only when
the two elements are of the same type; we shall have no need to compare elements of different types.

Here are the rules for constructing equality types.
\begin{enumerate}
\item[E1:]
  for any type $X$ and for any elements $a$ and $b$ of it, there is a type $a=b$;
\item[E2:] for any type $X$ and for any element $a$ of it, there is an element $\refl a$ of type $a=a$ (the name $\refl{}$ comes from the word
  ``reflexivity'')
\item[E3:] for any type $X$ and for any element $a$ of it, given a family of types $P(b,e)$ depending on parameters $b$ of type $X$ and $e$ of type
  $a=b$, in order to define elements $f(b,e) : P(b,e)$ of all of them it suffices to provide an element $p$ of $P(a,\refl a)$.  The resulting
  function $f$ may be regarded as having been completely defined by the single definition $f(a,\refl a) \defeq p$.
\end{enumerate}

We will refer to an element $i$ of $a=b$ as an {\em identification} of $a$ with $b$, because there may be more than one of them.  When we know
that there can be at most of them, we will refer to $i$ as a {\em proof} that $a$ is equal to $b$.

We see from rule E2 that $\refl{S(S(0))}$ serves as a proof of $\fact(2)=2$, 
as do $\refl{2}$ and $\refl{\fact(2)}$.  A student might wish for a
more detailed proof of that equation, but as a result of our convention above that definitions are syntactically transparent, the application of
definitions, including inductive definitions, is regarded as a trivial operation.

We will refer to rule E3 as ``induction for equality''.  It says that to prove something about (or to construct something from) every proof that
$a$ is equal to something else, it suffices to consider the special case where the proof is the trivial proof that $a$ is equal to itself, i.e.,
where the proof is $\refl a : a=a$.  Notice that the single case in such an induction corresponds to the single way of introducing elements of
equality types via rule E2, and compare that with P4, which dealt with the two ways of introducing elements of $\NN$.
%% ???
Intuitively, the induction principle for equality amounts to saying that the element $\refl a$ ``generates'' the system of types $a=b$, as $b$
ranges over elements of $A$.

We may use induction to prove symmetry of equality.  In accordance with our discussion of implication above, we show how to produce an element
of $b=a$ from an element $p$ of $a=b$, for any $b$ and $p$.  By induction (letting $P(b,e)$ be $b=a$ for any $b$ of type $X$ and for any $e$ of
type $a=b$, for use in rule E3 above), it suffices to produce an element of $a=a$; we choose $\refl a$ to achieve that.

Transitivity of equality is established the same way.  For each $a,b,c:X$ and for each $p:a=b$ and for each $q:b=c$ we want to produce an
element of type $a=c$.  By induction on $q$ we are reduced to the case where $c$ is $b$ and $q$ is $\refl b$, and we are to produce an element
of $a=b$.  The element $p$ serves the purpose.  
%Notice the similarity of this inductive definition with the definition given above 
%of the sum $m+n$. HMM ... (left versus right recursion)

Now we state our symmetry result a little more formally.

\begin{definition}\label{def:eq-symm}
  For any type $X$ and for any $a,b:X$, let $\symm_{a,b} : (a=b) \to (b=a)$ be the function defined by induction by setting
  $\symm_{a,a}(\refl a) \defeq \refl a$.
\end{definition}

Similarly, transitivity is formulated as an inductive definition for a function $\trans_{a,b,c} : (a=b) \to ((b=c) \to (a=c))$.  We may
abbreviate $(\trans_{a,b,c}(p))(q)$ as either $p\ct q$, 
or as $q\cdot p$, $qp$, or $q\circ p$.
(The latter notation will be used when $a,b,c$ are types and
$p$ and $q$ come from equivalences $a\to b$ and $b\to c$, respectively.
MAKE PICTURE)

Associativity of transitivity is formulated and established similarly.  
We leave that as an exercise.

One frequent use of elements of identity types is in \emph{substitution}.  Let $X$ be a type, and let $T(x)$ be a family of types depending on a
parameter $x:X$.  Suppose $x,y:X$ and $e:x=y$.  Then there is a function of type $T(x) \to T(y)$. We define one specific such function by induction, by taking its value to be the identity function on $T(x)$ in the case of $\refl{x}:x=x$.
\begin{definition}\label{def:transport} The function
  \[ 
  \trp_{T,e} : T(x) \to T(y)
  \]
  is defined by induction setting $\trp_{T,\refl{x}} (t) \defeq t$.
\end{definition} 
The function thus defined may be called 
\emph{the transport function in the type family $T$ along the path $e$}, 
 or, less verbosely, \emph{transport}.
 We may also simplify the notation to just $\trp_e$.
The transport functions behave as expected: transport along the composition
$e\cdot e'$ is the composition of the two transport functions (to be
 proved by induction).

When the types $T(x)$ may have more than one element, 
we may regard an element of $T(x)$ as providing additional {\em structure} on $x$. 
In that case, we will refer to the transport function $T(x) \to T(y)$ as 
\emph{transport of structure} from $x$ to $y$. 

Take, for example, $T(x)\defeq (x=x)$. 
Then $\trp_e$ is of type $x=x \to y=y$ and transports the
symmetries of $x$ to the symmetries of $y$.

By contrast, when the types
$T(x)$ have at most one element, we may regard an element of $T(x)$ 
as providing a proof of a property of $x$. In that case, the transport
function $T(x) \to T(y)$ provides a way to establish a claim about $y$ 
from a claim about $x$, so we will refer to it as \emph{substitution}.  In
other words, elements that can be identified have the same properties.



\section{Product types}
\label{sec:product-types}
Our type theory will also contain \emph{products} of types. 
By this we mean if $X$ is a type and $Y(x)$ is a family of types indexed by a
parameter $x$ of type $X$, then there will be a type $\prod_{x:X} Y(x)$ 
whose elements $f$ are functions that provide elements $f(a)$ of type
$Y(a)$, one for each $a:X$. We may refer to $X$ as the 
\emph{index type} of the product.

We have actually seen such functions already, for example,
in P4 in \cref{sec:natural-numbers}, where
we called $f(m):X(m)$ a `family of elements'. We can now
also say $f: \prod_{m:\NN} X(m)$. However, we will continue to use
phrases with `family' and `for all' as well, always referring to functions
of appropriate types.

A function $f : X \to Y$ is essentially the same thing as a function $f$ 
of type $\prod_{x:X} Y$, where the product is formed using a constant family of types.

The basic way of constructing an element of a product type
is by explicit definition, as we have seen before.

Functions preserve equality, a fact that is frequently used.
We make this fully precise now, for once and for all, 
to be used implicitly later on.

\begin{definition}\label{def:apd}
For all types $X$, $Y$, functions $f:X\to Y$ and elements $x,y:X$, the function
$\ap{f} : x = y \to f(x) = f(y)$ is defined by induction by setting 
$\ap{f}(\refl{x})\defeq\refl{f(x)}$. More generally, 
if $Z(x)$ is a type for every element $x:X$, and $f(x):Z(x)$
a dependent function, then we define the dependent function
$\apd{f}$ such that for every $e:x=y$
\[
 \apd{f}(e): (\trp_{Z,e}(f(x))) = f(y);~\apd{f}(\refl{x})\defeq\refl{f(x)}.
\]
This is well-typed since $\trp_{Z,\refl{x}}(f(x)) \jdeq f(x)$. 
\end{definition}

If two functions $f$ and $g$ of type $\prod_{x:X} Y(x)$ are equal, 
then they have equal values, i.e., for every element $x$ of $X$, 
we may conclude that $f(x) = g(x)$.
This can be proven by induction or by substitution.
\begin{xca}\label{happly}
Let $f,g:\prod_{x:X} Y(x)$. Define a function of type 
\[ 
f=g \to \prod_{x:X} f(x)=g(x).
\] 
\end{xca}

Conversely, a basic principle called `function extensionality'
asserts that $f=g$ whenever $f(x) = g(x)$ for every element $x$ of $X$.

\section{Inductive types}
\label{sec:inductive-types}

There are other examples of types that are conveniently presented as 
inductive definitions, in the style we have seen with the natural numbers
and the equality types.  We start by the finite types, and then
present several constructions defining new types from old ones.
For each of these constructions we explain what it means for two 
elements of the newly constructed type to be equal in terms of
equality in the constituent types.

\subsection{Finite types}
\label{sec:finite-types}
Firstly, there is the ``empty'' type, denoted $\bn{0} $, defined inductively, with no way to construct elements provided in the inductive
definition.  The inductive principle for $\bn{0} $ says that to prove something about (or to construct something from) every element of
$\bn{0} $, it suffices to consider no special cases (!).  Hence, every statement about an arbitrary element of $\bn{0} $ can be proven. (This is called the Ex Falso
rule in traditional logic.) As
an example, we may prove that any two elements $x$ and $y$ of $\bn{0} $ are equal by using induction on $x$.

An element of $\bn{0} $ will be called an \emph{absurdity}, and the negation $\neg P$ of a proposition $P$ will be implemented as the function
type $P \to \bn{0} $.  This is sensible, because an element of $\neg P$ could be applied to an element of $P$ to produce an element of
$\bn{0} $, i.e., an absurdity.

Another appropriate name for $\bn{0} $ is $\false$.

We may also construct a function $\false \to X$, for any type $X$, by induction, showing that from an absurdity anything follows.

To encode the property that $X$ has no elements we use the type $X \to \bn{0} $.  To encode the property that elements $a,b:X$ are not equal,
we use the type $(a=b) \to \bn{0} $, and we let $a \ne b$ denote it.

Secondly, there will also be a type called $\true$, defined inductively and provided with a single element $\triv$; (the name $\triv$ comes from the word
  ``trivial'').  Its induction principle
states that, in order to prove something about (or to construct something from) every element of $\true$, it suffices to consider the special
case where the element is $\triv$.  As an example, we may prove, for any element $u : \true$, that $u=\triv$, by using induction to reduce
to proving $\triv=\triv$, a proof of which is provided by $\refl{\triv}$.  One may also prove that any two elements of $\true$ are equal by using induction twice.

There is a function $X \to \true$, for any type $X$, namely: $a \mapsto \triv$.  This corresponds, for propositions, to the statement that an
implication holds if the conclusion is true.

Thirdly, there will be a type called $\bool$, defined by induction and provided with two elements, $\yes$ and $\no$.  One may prove by induction
that any element of $\bool$ is equal to $\yes$ or to $\no$.

We may use substitution to prove $\yes \ne \no$.  To do this, we introduce a family of types $P(b)$ parametrized by a variable $b:\bool$.
Define $P(\yes) \defeq \true$ and define $P(\no) \defeq \false$.  The definition of $P(b)$ is motivated by the expectation that we will be able
to prove that $P(b)$ and $b = \yes$ are equivalent.  If there were an element $e: \yes = \no$, we could substitute $\no$ for $\yes$ in $\triv :
P (\yes)$ to get an element of $P(\no)$, which is absurd.  Since $e$ was arbitrary, we have defined a function $(\yes=\no) \to \bn{0} $,
establishing the claim.

In the same way, we may use substitution to prove that successors of natural numbers are never equal to $0$, i.e., for any $n:\NN$ that $0 \ne
S(n)$.  To do this, we introduce a family of types $P(i)$ parametrized by a variable $i:\NN$.  Define $P$ recursively by specifying that $P(0)
\defeq \true$ and $P(S(m)) \defeq \false$.  The definition of $P(i)$ is motivated by the expectation that we will be able to prove that $P(i)$
and $i = 0$ are equivalent.  If there were an element $e: 0 = S(n)$, we could substitute $S(n)$ for $0$ in $\triv : P ( 0 )$ to get an element
of $P(S(n))$, which is absurd.  Since $e$ was arbitrary, we have defined a function $(0=S(n)) \to \bn{0} $, establishing the claim.

In a similar way we will in \cref{sec:typeFin} define types $P(n)$ for any $n$ in $\NN$
such that $P(n)$ is the type (set) of $n$ elements.

\subsection{Sum types}
\label{sec:sum-types}
There are \emph{sums} of types.  By this we mean if $X$ is a type and $Y(x)$ is a family of types indexed by a parameter $x$ of type $X$, then
there will be a type $\sum _{x:X} Y(x)$ whose elements are all pairs $(a,b)$, where $a:X$ and $b:Y(a)$. Since the type of $b$ may depend on $a$ we also call such a pair
a \emph{dependent} pair. We may refer to $X$ as the \emph{index
  type} of the sum.  

Proving something about (or constructing something from) every 
element $(a,b)$ of $\sum _{x:X} Y(x)$ is simply done for all $a:X$ and $b: Y(a)$.
Two important examples of such constructions are:
\begin{enumerate}
\item \emph{first projection}, 
$\fst:(\sum _{x:X} Y(x)) \to X$, 
$\fst(a,b)\defeq a$;
\item \emph{second projection},
$\snd(a,b): Y(a)$,
$\snd(a,b)\defeq b$.
\end{enumerate}
In (2), the type of $\snd$ is in full
$\prod_{z:\sum_{x:X} Y(x)} Y(\fst(z))$.

Equality of two elements $(a_1,b_1),(a_2,b_2)$ of $\sum _{x:X} Y(x)$ is 
inductively defined in \cref{sec:identity-types}, like for any other type.
In the case of a structured type defined from other types, 
here $\sum _{x:X} Y(x)$ from $X$ and $Y(x)$ for any $x$,
one would like to express equality in the structured type in terms of 
the equalities in the constituent types. This explains much better
what it means for two elements of the structured type to be equal.
This is so important that we give this explanation here and below
every time we introduce a new structured type, even though not everything
can be proved yet. For complete proofs we refer to \cite{hottbook}.

An identification between two elements $(a_1,b_1),(a_2,b_2)$ of 
$\sum _{x:X} Y(x)$ means in the first place, by taking
the first projection, having an identification $i: a_1=a_2$.
We cannot expect exactly the same for the second 
projections $b_1: Y(a_1)$ and $b_2: Y(a_2)$, since they may
have different types and can therefore not be compared directly.
However, after transport of $b_1$ in the type family $Y$
along the identification $i: a_1=a_2$, a direct comparison to $b_2$
is possible. Thus we obtain that each identification $(a_1,b_1)=(a_2,b_2)$
can be viewed as a pair of an identification $i: a_1=a_2$ and an
identification $i' : \trp_{Y,i}(b_1) = b_2$. Note that the type
of $i'$ depends on $i$, so that we actually have a dependent pair:
\[
(i,i'): \sum_{i:a_1 = a_2} \trp_{Y,i}(b_1)= b_2
\]
Conversely, we can define by induction a function of
\[
(\sum_{i:a_1 = a_2} \trp_{Y,i}(b_1)= b_2) \to (a_1,b_1)=(a_2,b_2).
\]
which often helps proving dependent pairs equal.
See Theorem 2.7.2 in \cite{hottbook} for a detailed account.


\subsection{Binary products}
\label{sec:binprod-types}
There is special case of sum types that deserves to be mentioned since
it occurs quite often. Let $X$ and $Y$ be types, and consider the constant
family of types $Y(x)\defeq Y$. In other words, $Y(x)$ is a type that depends
on an element $x$ of $X$ that happens to be $Y$ for any such $x$.
Then we can form the sum type $\sum_{x:X} Y(x)$ as in the previous
section \ref{sec:sum-types}. Elements of this sum type are pairs $(x,y)$
with $x$ in $X$ and $y$ in $Y(x)\jdeq Y$. In this case the type of $y$
doesn't depend on $x$, and in this special case the sum type is called
the \emph{binary product}, or \emph{cartesian product} of the types $X$ and $Y$,
denoted by $X \times Y$.

Recall that we have seen something similar with the product type
$\prod_{x:X} Y(x)$, which we denote $X\to Y$ in case $Y(x)\defeq Y$.
The type $X \times Y$ inherits the functions $\fst,\snd$ from
$\sum_{x:X} Y(x)$, with the same definitions $\fst(x,y)\defeq x$
and $\snd(x,y)\defeq y$. Their types can now be denoted in a
simpler way as $\fst: (X \times Y)\to X$ and 
$\snd: (X \times Y)\to Y$, and they are called as before the
first and the second projection, respectively.

Again, proving something about (or constructing something from) every 
element $(a,b)$ of $X \times Y$ is simply done for all $a:X$ and $b:Y$.
An identification between two elements $(a_1,b_1),(a_2,b_2)$ of 
$X \times Y$ consists of an identification of $a_1$ and $a_2$ in $X$
and an identification of $b_1$ and $b_2$ in $Y$. In fact, 
again, the type of identifications between two pairs in $X \times Y$
is equivalent to the binary product of the types of identifications between
elements of $X$ and of identifications between elements of $Y$.

\subsection{Binary sums}
\label{sec:binsum-types}
If a sum type is of the form $\sum_{b:\bool} T(b)$, with $T(b)$
a type depending on $b$ in $\bool$, there is a simpler way of
describing it. After all, the type family $T(b)$ is fully determined
by two types, namely by the types $T(\no)$ and $T(\yes)$.
The elements of $\sum_{b:\bool} Y(b)$ are dependent pairs $(\no,a)$ with
$a$ in $T(\no)$ and $(\yes,b)$ with $b$ in $T(\yes)$. The resulting
type can be viewed as the \emph{disjoint union} of $T(\no)$ and $T(\yes)$:
from an element of $T(\no)$ or an element of $T(\yes)$ 
we can produce an element of $\sum_{b:\bool} T(b)$.  

Such types can be described more clearly in the following way.
The \emph{binary sum} of two types $X$ and $Y$, denoted $X \amalg Y$,
is an inductive type with two constructors: $\inl{} : X \to X \amalg Y$ and
$\inr{} : Y \to X \amalg Y$. Proving a property of any element of $X \amalg Y$
means proving that this property holds of any $\inl{x}$ with $x:X$ and any
$\inr{y}$ with $y:Y$. In general, constructing a function $f$ of type
$\prod_{z: X \amalg Y} T(z)$, where $T(z)$ is a type depending on 
$z$, is done by defining $f(\inl{x})$ for all $x$ in $X$
and $f(\inr{y})$ for all $y$ in $Y$.

Identification of two elements $a$ and $b$ in $X \amalg Y$ is 
only possible if they are constructed with the same constructor.
Thus $\inl{x} = \inr{y}$ is always empty, and identifications
$\inl{x} = \inl{x'}$ are equivalent to identifications $x=x'$ in $X$,
and identifications
$\inr{y} = \inr{y'}$ are equivalent to identifications $y=y'$ in $Y$.

\section{Equivalences}\label{sec:equivalence}


The combination of $\sum$-, $\prod$- and equality types allows
us to express important notions, as done in the following
definitions.

\begin{definition}
\label{def:contractible}
Let $X$ be a type. 
The property that $X$ has exactly one element may be expressed by saying $X$ has an element such that every other element is
equal to it.  Hence it is encoded by the type $\sum_{c:X} \prod_{x:X} (c=x)$.
We call a type $X$ having this property \emph{contractible}, 
denoted by $\iscontr(X)$, and $c$ its \emph{center}. 
(Note that the notion of center depends on the proof that $X$
is contractible, rather than on the type $X$. 
Any element of a contractible type $X$ can be proved to be a center.) 
\end{definition}

An important example of a contractible type is the
\emph{singleton type} $\sum_{x:X} (a=x)$, thought of as
the subtype of the type $X$ consisting of the element $a$.
In order to see that singleton types are contractible,
take as center the element $(a,\refl{a})$. We have
to prove for any element $x$ of $X$ and identification
$e: a=x$ that $(a,\refl{a}) = (x,e)$. This we do componentwise,
as explained in \cref{sec:sum-types}. For the identification of
the first components we take (of course) $e$. For the second
components, we have to prove $\trp_{a=\_,e}\refl{a}= e$.
This follows immediately by induction on $e$.
%EASY: for $e\jdeq \refl{a}$ take $\refl{\refl{a}} : trp_{a=\_,e} = e$.

\begin{definition}
\label{def:fiber}
Given a function $f : X \to Y$ and an element $y:Y$, the \emph{fiber} (or \emph{inverse image}) $f^{-1}(y)$ consists of elements $x$ such that $f(x)
= y$.  This is encoded by defining $f^{-1}(y) \defeq \sum_{x:X} (f(x) = y)$.  In other words, an element of the fiber is a pair $(x,e)$ consisting
of an element $x$ and an element $e$ of the identity type $f(x) = y$.
\end{definition}

In set theory, a function $f : X \to Y$ is a bijection if and only if
all inverse images $f^{-1}(y)$ consist of exactly one element.
This we can also express in type theory, in an definition due
to Voevodsky. 

\begin{definition}
\label{def:equivalence}
A function $f : X \to Y$ is called an \emph{equivalence},
denoted by $\isEq(f)$, if $\iscontr(f^{-1}(y))$ for all $y:Y$.
\end{definition}

Let $t(y): \iscontr(f^{-1}(y))$ for all $y:Y$.
Using $t$ we can define a canonical inverse function
%that we (abusively) also denote by $f^{-1}$,
by setting $f^{-1}(y)\defeq\fst(\fst(t(y)))$.
Note that we use $f^{-1}$ both for the fibers and
for the inverse function in case the fibers are contractible.
The dependency of $f^{-1}$ on $t$ is harmless by the very nature of
being contractible: different $t$'s would give the same
function by the principle of extensionality.

Clearly, $f(f^{-1}(y)) = y$ by unfolding all definitions.
Moreover, we have $(x,refl{f(x)}) : f^{-1}(f(x)$,
with the latter as the \emph{fiber} that $t(f(x))$
proves contractible. Hence the center of contraction
$\fst(t(f(x))$ is equal to $(x,refl{f(x)})$, and so
$f^{-1}(f(x))\jdeq(\fst(\fst(t(f(x))) = x$.

In the above case we say that the types
$X$ and $Y$ are \emph{equivalent}, denoted by $X\equiv Y$. 
More precisely, since there could be more than one equivalence
between two types, we define the type of equivalences from $X$ to $Y$ by
\[
(X\equiv Y) \defeq \sum_{f:X\to Y} \isEq(f) 
\]


\begin{xca}\label{xca:fstfstcontractiblefiber}
Make sure you understand the two applications of $\fst$
in the definition $f^{-1}(y)\defeq\fst(\fst(t(y)))$ above.
\end{xca}

\begin{xca}\label{xca:equivalence-invers}
Show that $f^{-1}$ is an equivalence from $Y$ to $X$.
\end{xca}

\begin{xca}\label{xca:equivalencerel-equivalence}
Show that $\equiv$ is an equivalence relation on types.
\end{xca}

For any type $X$, the identity function $\id_X$ is an
equivalence from $X$ to $X$: for every element $a$ in $X$,
$\id_X^{-1}(a)$ is a singleton type and hence contractible.
This simple observation combined with the fact that
$\trp_{T,\refl{x}}\jdeq \id_{T(x)}$ gives us the next
lemma by induction.

\begin{lemma}\label{lem:equivalence-transport}
Let $X$ be a type, and let $T(x)$ be a type depending on $x:X$.
Then for every $x,y$ in $X$ and $e: x=y$, the function $\trp_{T,e}$
is an equivalence from $T(x)$ to $T(y)$, so $T(x)\equiv T(y)$.
\end{lemma}

The following lemma gives an equivalent characterization
of equivalence that is sometimes easy to use.

\begin{lemma}\label{lem:equivalence-via-inverse}
Let $X,Y$ be types. A function $f: X\to Y$ is an equivalence
if and only if there exists a function $g: Y\to X$ such that
for all $x:X$ we have $g(f(x))=x$ and for all 
$y:Y$ we have $f(g(y))=y$.
\end{lemma}
\begin{proof}
If $f: X\to Y$ is an equivalence we can take $g=f^{-1}$.
For the converse, see
\cite[Chapter]{hottbook}, or
\href{https://github.com/UniMath/UniMath/}{\url{Unimath/Foundations/PartA.v, isweq_iso}}.
OK? VVs PROOF IS PERFECT BUT WILL THE LINK BE STABLE?
\end{proof}

\section{Universes and univalence}\label{sec:univax}

Univalence expresses, in a way made precise below, that
equivalent types are equal. Discussing $X = Y$, 
like any other identity (see \cref{sec:identity-types}),
requires an ambient type containing the subjects of the identification,
here the types $X$ and $Y$.
In other words, we need a type of types.

\begin{definition}\label{def:universe}
There is type, denoted $\UU$ and called a \emph{universe}, that contains
all types discussed so far, with the exception of $\UU$ itself and the types 
$X = Y$, where $X$ and $Y$ themselves are types.
\end{definition}

Using $\UU$ we can now also assign a type to type families:
if $Y(x):\UU$ for all $x:X$, $X:\UU$, then $Y$ has type $X\to\UU$.
What is the type of $X\to\UU$, and of $\UU$ itself?
The answer is: the next universe.
In this book we use, mostly implicitly, as many such universes as we need.
Note that a countable linear hierarchy of universes achieves
the foundational ideal of `everything has a type' while
avoiding the paradoxical $\UU:\UU$.

The univalence axiom is a statement about the universe $\UU$: 
for all types $X$ and $Y$ in $\UU$ being \emph{equal} in $\UU$ is 
\emph{equivalent} to them being \emph{equivalent}. This greatly
enhances our capacity to prove that two types are equal
and to use this equality to transport properties and structure
between them. We deliberately use the term transport
here since it is precisely transport in the sense of
\cref{def:transport} along the identification
$X = Y$ stipulated by the equivalence $f$.

We first define the \emph{function} that
the univalence axiom postulates to be an equivalence.

\begin{definition}\label{def:idtoeq}
Define the identity function $id_\UU$ on $\UU$ by $\id_\UU(X)\defeq X$ 
for all types $X$ in $\UU$.
We can alternatively view $\id_\UU$ as a type family on $\UU$. 
Under the latter view,
given an identification $e : X = Y$ in $\UU$, we can
transport elements from $X$ to elements of $Y$ using the transport
function from \cref{def:transport},
$\trp_{\id_\UU, e} : X \to Y$.
By \cref{lem:equivalence-transport}, $\trp_{\id_\UU, e}$ is an 
equivalence from $X$ to $Y$. 
Together, they define the function
\[
\idtoeq : (X = Y) \to (X\equiv Y).
\]
\end{definition}

Now comes the univalence axiom.

\begin{definition}\label{def:univalence}
Voevodsky's \emph{univalence axiom} (UA) postulates that 
$\idtoeq$ above is an equivalence for all $X,Y:\UU$.
Formally: \[ ua : \isEq(\idtoeq).\]
\end{definition}

This formulation is a bit `heavy' for daily use,
so we elaborate.
Let $p$ be a proof that $f: X\to Y$ is an equivalence.
Then, unfolding $\isEq$ according to \cref{def:equivalence}, 
$ua(f,p): \iscontr(\idtoeq^{-1}(f,p))$.
Unfolding $\iscontr$ according to \cref{def:contractible}, 
$\fst(ua(f,p))$ is the center of contraction 
of the fiber $\idtoeq^{-1}(f,p)$,
say a pair $(e,q)$ with $e:X=Y$ and $q: \idtoeq(e)=(f,p)$.
Then $\fst(\fst(ua(f,p)))\jdeq e$. 
The function $\fst\circ\fst\circ {\,ua}$ will
by an abuse of notation also be written as $ua$:
\[
ua : (X \equiv Y) \to (X=Y). 
\]
By an even larger abuse we denote the identification
$\fst(\fst(ua(f,p)))$ also by $f: X=Y$. 
The larger abuse of notation is compatible with the following
`practical use' of UA: 
\begin{quote}
Transport from $X$ to $Y$ along the identification of $X$ and $Y$
coming from an equivalence $f : X\to Y$ through UA is just applying $f$.
\end{quote}
In order to understand this practical form of UA,
let $p$ be a proof that $f$ is an equivalence.
As above, $\fst(ua(e,p)) = (e,q)$ with $q: \idtoeq(e)=(f,p)$.
Using \cref{def:idtoeq}, $\fst(\idtoeq(e))\jdeq\trp_{\id_\UU, e}$,
so actually $\trp_{\id_\UU, e} =f$ using $q$.

The notations and conventions are chosen in such a way that,
if you start with an equivalence $f : X\to Y$ and you go all 
the way to transport from $X$ to $Y$ along the identification
$f: X=Y$ given by UA, you end up applying this same $f$, 
and you never have to change notation underway. 
It is, however, important to stress that the above pattern is
not the only way UA can be applied.

\begin{xca}\label{xca:C2}
   Prove that $\mathbf 2=\mathbf 2$ has exactly two elements, $\refl{\mathbf 2}$ and $\twist$ (where $\twist$ is given by univalence from the equivalence $\mathbf 2\to\mathbf 2$ exchanging the two elements of $\mathbf 2$), and that $\trans{}_{\mathbf 2,\mathbf 2,\mathbf 2}(\twist)(\twist)=\refl{\mathrm 2}$.
\end{xca}

% \begin{remark} % BID
%   The lack of symmetry (why \emph{left} inverse?) in this formulation is unsatisfactory and avoidable at the price of having a slightly more elaborate definition of $\Eq(A,B)$.\footnote{contractible fibers definition is also lop-sided in the sense that you define a section (left inverse) by picking the center of the contraction}
% \end{remark}

\section{Propositions, sets and groupoids}
\label{sec:props-sets-grpds}

Let $X$ be a type.  The property that $X$ has at most one element may be expressed by saying that any two elements are equal. Hence it is encoded
by $\prod_{a,b:X} (a=b)$.  We shall call such a type a \emph{proposition}, and its elements will be called \emph{proofs}.

Let $X$ be a type.  If for any $x:X$ and any $y:X$ the identity type $x=y$ is a proposition, then we shall say that $X$ is a \emph{set}.
Alternatively, we shall say that $X$ is a 0-\emph{type}.\footnote{%
Sets are thought to consist of points. Points are entities of dimension 0, 
which explains why the count starts here.
One of the contributions of Vladimir Voevodsky is the extension of
the hierarchy downwards, with the notion of proposition,
including logic in the same hierarchy.
Some authors therefore call propositions $(-1)$-\emph{types}.} 

Let $X$ be a type.  If for any $x:X$ and any $y:X$ the identity type $x=y$ is a set, 
 then we shall say that $X$ is a \emph{groupoid}, also called a 1-\emph{type}.

The pattern continues.  If for any $n:\NN$, any $x:X$, and any $y:X$ 
the identity type $x=y$ is is an $n$-\emph{type}, 
then we shall say that $X$ is an $(n+1)$-\emph{type}.

We prove that every proposition is a set, from which it follows
by induction that every $n$-type is an $(n+1)$-\emph{type}.

\begin{lemma}\label{lem:prop-is-set}
Every type that is a proposition is also a set.
\end{lemma}
\begin{proof}
Let $X$ be a type and let $f: \prod_{a,b:X} (a=b)$. Let $a,b,c : X$ and
let $P(x)$ be the type $a=x$ depending on $x:X$. Then
$f(a,b):P(b)$ and $f(a,c):P(c)$. By path induction we prove for
all $q:b=c$ that $q\cdot f(a,b) = f(a,c)$. For this it suffices to
verify that $\refl{b} \cdot f(a,b) = f(a,b)$, which follows immediately.
So $q$ is equal to $f(a,c)\cdot f(a,b)^{-1}$ which doesn't
depend on $q$, so all such $q$ are equal. Hence $X$ is a set.
\end{proof}

In the following lemma we collect a number of useful results on propositions.

\begin{lemma}\label{lem:prop-utils}
Let $A$ be a type, and let $P$ and $Q$ propositions.
Let $R(a)$ be a proposition depending on $a:A$. Then we have:
\begin{enumerate}
\item\label{prop-utils-false-true} $\false$ and $\true$ are propositions;
\item\label{prop-utils-implication} $A\to P$ is a proposition;
\item\label{prop-utils-pi} $\prod_{a:A} R(a)$ is a proposition;
\item\label{prop-utils-times} $P\times Q$ is a proposition;
\item\label{prop-utils-eq} $P \liff Q$ is a proposition.
\end{enumerate}
\end{lemma}

\begin{proof}
(\ref{prop-utils-false-true})
If $p,q : \false$, then $p=q$ is proved by the Ex Falso rule.
If $p,q : \true$, then $p=q$ is proved by double induction,
which reduces the proof to observing that $\refl{\triv}: \triv=\triv$.

(\ref{prop-utils-implication})
If $p,q : A\to P$, then $p=q$ is proved by first observing that $p$ and $q$
are functions which, by function extensionality, are equal if they have
equal values $p(x) = q(x)$ in $P$ for all $x$ in $A$. This is
actually the case since $P$ is a proposition.

(\ref{prop-utils-pi})
If $p,q : \prod_{a:A} R(a)$ one can use the same argument as for $A\to P$
but now with \emph{dependent} functions $p,q$.

(\ref{prop-utils-times})
If $(p_1,q_1),(p_2,q_2) : P\times Q$, then $(p_1,q_1)=(p_2,q_2)$
is proved componentwise. 

(\ref{prop-utils-eq})
Using \cref{lem:equivalence-via-inverse}, $P \liff Q$ is equivalent to
$(P\to Q)\times(Q\to P)$, which is a proposition by 
combining (\ref{prop-utils-implication}) and (\ref{prop-utils-times}).
\end{proof}

Several remarks can be made here. First, the lemma supports the
use of $\false,\true$ as truth values, and the use of
$\to,\prod,\times$ for implication, universal quantification,
and conjunction, respectively. Since $\false$ is a proposition,
it follows by (\ref{prop-utils-implication}) above that
$\neg A$ as defined by $A\to\false$ is a proposition for any type $A$.
As noted before, (\ref{prop-utils-implication}) is a
special case of (\ref{prop-utils-pi}).

Notably absent in the lemma above are disjunction
and existential quantification. This has a simple reason:
$\true\amalg \true$ has the distinct elements
$\inl{\triv}$ and $\inr{\triv}$, an is therefore \emph{not} a proposition.
Similarly, $\sum_{n:\NN} \true$ has infinitely many
distinct elements $(n,\triv)$ and is not a proposition. We will see later how
to work with disjunction and existential quantification for propositions.

The lemma above has a generalization from propositions to
$n$-types which we state without proving.

\begin{lemma}\label{lem:level-n-utils}
Let $A$ be a type, and let $X$ and $Y$ be $n$-types.
Let $Z(a)$ be an $n$-type depending on $a:A$. Then we have:

\begin{enumerate}
\item\label{level-n-utils-implication} $A\to X$ is an $n$-type;
\item\label{level-n-utils-pi} $\prod_{a:A} Z(a)$ is an $n$-type;
\item\label{level-n-utils-times} $X\times Y$ is an $n$-type.
\end{enumerate}
\end{lemma}

We recall the following definitions.
\begin{align*}
\isprop(P) &\defeq\prod\nolimits_{p,q:P}(p=q)\\
\isset(S) &\defeq\prod\nolimits_{x,y:S}\isprop(x=y)\jdeq
                  \prod\nolimits_{x,y:S}\prod\nolimits_{p,q:(x=y)}(p=q)\\
\isgrpd(G) &\defeq\prod\nolimits_{g,h:G}\isset(g=h)\jdeq \ldots\\
\end{align*}

\begin{lemma}\label{lem:isX-is-prop}
  For any type $A$, the types 
$\iscontr(A)$, $\isprop(P)$, $\isset(S)$, $\isgrpd(G)$, etc.\ are propositions.
(We shall only use the notation $\mathrm{isX}(A)$ for propositions about $A$.)
\end{lemma}

\begin{proof}
Recall that $\iscontr(A)$ is $\sum_{a:A} \prod_{b:A} (a=b)$.
Let $(a,f)$ and $(b,g)$ be elements of the type $\iscontr(A)$.
For $(a,f) = (b,g)$ we need an $e : a=b$ and an $e' : \trp_e f = g$.
For $e$ we can take $f(b)$. Clearly, $A$ is a proposition and hence
a set by \cref{lem:prop-is-set}. Hence the type of $g$ is a proposition
by \cref{lem:prop-utils}(\ref{prop-utils-pi}), which gives us $e'$.

We delegate the other (and future) cases to the exercises.
\end{proof}

\begin{xca}\label{xca:isX-is-prop}
Make sure you understand that $\isprop(P)$ is a proposition,
using the same lemmas as for $\iscontr(A)$.
Show that $\isset(S)$ and $\isgrpd(G)$ are propositions.
\end{xca}

\section{Heavy transport}
\label{sec:heavy-transport}

In this section we collect useful results on transport in
type families that are defined by a type constructor applied
to types that are depending on an element of some index type.
Typical examples of such `structured' type families are 
$Y(x)\to Z(x)$ and $x=x$ depending on $x:X$.

\begin{lemma}\label{lem:transport-in-function-type}
Let $X$ be a type, and let $Y(x)$ and $Z(x)$ be types for every $x:X$.
Let $Y\to Z$ be the type family such that $(Y\to Z)(x) \defeq Y(x)\to Z(x)$.
Then we have for every $x,x':X$, $e: x=x'$, $f: Y(x)\to Z(x)$, and $y':Y(x')$:
\[
\trp_{Y\to Z, e} (f)(y') = \trp_{Z,e} (f(\trp_{Y, e^{-1}}(y')).
\]
\end{lemma}
\begin{proof}
By induction on $e: x=x'$. For $e \jdeq \refl{x}$ we have $e^{-1}\jdeq \refl{x}$
and all transports are identity functions of appropriate type. 
\end{proof}

An important special case of the above lemma is with $\UU$
as index type and type families $Y\defeq Z\defeq \id_\UU$.
Then $Y\to Z$ is $X\to X$ as type depending on $X:\UU$. Now, 
if $A:\UU$ and $e: A=A$ comes by UA from some equivalence 
$g:A\to A$, then the above lemma combined with function extensionality 
yields that for any $f: A\to A$
\[
%\trp_{X \mapsto (X\to X), e} (f) = g\circ f \circ g^{-1}.
\trp_{(\id_\UU\to \id_\UU), e} (f) = g\circ f \circ g^{-1}.
\]
This equation is phrased as `transport by conjugation'.

\begin{xca}\label{xca:conjugation}
Draw a commutative diagram for the above equation. PERHAPS I SHOULD
\end{xca}

\section{Logical operations on propositions; Propositional truncation}
\label{sec:logical-operations}

In \cref{sec:props-sets-grpds} we have seen that propositions are
closed under $\times$, $\to$ and taking products over arbitrary
index types. Moreover, $\false$ and $\true$ are propositions.
In this section we explain in detail how to do logic in type theory.
First, logical propositions are represented by the types that we
have already called propositions, that is, the types in which all
elements are equal. The reason for doing so is that the interesting
thing about a logical proposition is whether it has a proof or not.
It is therefore reasonable to require for any type representing 
a logical proposition that all its members are equal.

We have also seen that ${\amalg}$ and $\Sigma$ can lead to types
with distinct elements even though the constituents are
propositions. In order to enforce that all elements are equal
we define an operation called propositional truncation.\footnote{%
Misnomer because ... }
This is the first example of a \emph{higher inductive type}.
Higher inductive types are inductive types that also have
constructors for identifications between elements and/or
identifications between identifications.

\begin{definition}\label{def:prop-trunc}
Let $T$ be a type. The \emph{propositional truncation} of $T$
is a type  $\Trunc{T}$ defined by the following constructors:
\begin{enumerate}
\item an \emph{element} constructor $\trunc{\_} : T \to\Trunc{T}$
\item an \emph{identification} constructor $i: \prod_{x,y:\Trunc{T}} x=y$
\end{enumerate}
\end{definition}
Analogously to ordinary inductive types, any higher inductive type
comes with an induction principle that states how to prove something
about (or to construct something from) every element of the higher inductive type.
The induction principle should now also take into account 
the identification constructors. For the moment we limit
ourselves to giving a recursion principle stating how to define
a function $f$ of type $\Trunc{T} \to X$. The following data
suffices to specify such $f$:
\begin{enumerate}
\item a function $g$ of type $T\to X$ replacing $\trunc{\_}$; 
\item a function $\hat{\imath}$ of type $\prod_{x,y:X} x = y$ replacing $i$.
\end{enumerate}
These data yield a function $f$ with $f(\trunc{x})\defeq g(x)$ 
for all $x:T$. The function $\hat{\imath}$ warrants that $X$ is a
proposition so that $f$ maps equal elements of $\Trunc{T}$ to equal
elements of $X$.

Wanted:
\[
(\Trunc{A} \to B) \equiv (A \to B) \text{for $B$ a proposition}
\]



$\Trunc{A \amalg B}$
\section{Truncation}
\label{sec:truncation}

\section{Operations on sets that produce new sets}
\label{sec:operations-on-sets}

\begin{lemma}\label{lem:subtype}
Let $T$ be an $n$-type, and let $P(x)$ be a proposition depending on $x:T$. 
Then $\sum_{x:T} P(x)$ is also an $n$-type.
\end{lemma}

\begin{proof}
If $(x_1,p_1),(x_2,p_2) : \sum_{x:T} P(x)$, then by WHICH LEMMA
$(x_1,p_1)=(x_2,p_2)$ is equivalent to 
$\sum_{q:x_1=x_2} ({\trp}^P q\,p_1 = p_2)$. 
Since $P(x_2)$ is a proposition, using SEVERAL LEMMAS we get 
\[
%(x_1,p_1)=(x_2,p_2) \equiv
(\sum_{q:x_1=x_2} {\trp}^P q\,p_1 = p_2) \equiv 
(\sum_{q:x_1=x_2} \true) \equiv (x_1=x_2).
\]
Hence $\sum_{x:T} P(x)$ is an $n$-type if $T$ is.
\end{proof}
In the special case that $T$ is set we call 
$\sum_{x:T} P(x)$ a subset which we denote by 
$\set{x:T \mid P(x)}$.

\begin{lemma}\label{lem:eq_of_sets-is-set}
If $X$ and $Y$ are sets, then $X=Y$ is a set. 
In other words, $\Set$ is a groupoid.
\end{lemma}

\begin{proof}
By univalence, $(X=Y) \equiv (X\equiv Y)$. The latter type is
$\sum_{f:X\to Y} \isEq(f)$. Since $X$ and $Y$ are sets,
so is $X\to Y$ by \cref{lem:level-n-utils}. Moreover,
$\isEq(f)$ is a proposition by \cref{lem:isX-is-prop}.
It follows by \cref{lem:subtype} that $X=Y$ is a set.  
\end{proof}

One may wonder whether $\NN$ as defined in \cref{sec:inductive-types}
is a set. The answer is yes, but it is harder to prove than one
would think. In fact we have the following theorem.

\begin{theorem}\label{thm:isset-inductive-types}
All inductive types in \cref{sec:inductive-types} are sets
if all constituent types are sets.
\end{theorem}

\begin{proof}
We only do the case of $\NN$ and leave the other cases to the reader
(cf.\ \cref{xca:set-sum}). We have to prove that $n=m$ is a proposition
for all $n,m:\NN$. A simple induction starting with proving that $0=0$
is a proposition does not work. Instead we define by double induction
on $n,m:\NN$ a type $\Eq_\NN(n,m)$ that is propositional and equal 
to the type $n=m$.
We set $\Eq_\NN(0,0)\defeq\bn{1}$ and 
$\Eq_\NN(0,S(m))\defeq\Eq_\NN(S(n),0)\defeq\bn{0}$ for all $n,m:\NN$.
We set $\Eq_\NN(S(n),S(m))\defeq\Eq_\NN(n,m)$. 
By double induction on $n,m:\NN$ we get that $\Eq_\NN(n,m)$ is a proposition.
From this definition it also
follows by induction that $\Eq_\NN(n,n)=\bn{1}$ for all $n$. 
Again by induction on $n:\NN$ we define $e(n):\Eq_\NN(n,n)$ by 
setting $e(0)=0$ (well-typed since $0:\bn{1}\jdeq\Eq_\NN(0,0)$
and $e(S(n))\defeq e(n)$ (also well-typed).

Now it suffices to give equivalences $f_{n,m}: (n=m)\to\Eq_\NN(n,m)$.
We define $f$ by induction setting $f_{n,n}(\refl{n})\defeq e(n)$.
In order to prove that each $f_{n,m}$ is an equivalence, it is
convenient to define functions $g_{n,m}$ in the other direction. 
This we do by double induction on $n,m:\NN$. In the base case
we set $g_{0,0}(\_)=\refl{0}$. In the two cases $0,S(\_)$
and $S(\_),0$ for $n,m$ we have $\Eq_\NN(n,m)=\bn{0}$ 
and define $g_{n,m}: \Eq_\NN(n,m)\to (n=m)$ by Ex Falso.
In the last case we set $g_{S(n),S(m)}(x)=\ap{S} g_{n,m}(x)$,
well-typed for all $x: \Eq_\NN(S(n),S(m))\defeq\Eq_\NN(n,m)$.
By induction on $n:\NN$ we get $g_{n,n}(\_)=\refl{n}$.
By induction on $p: n=m$ we get $g_{n,m}(f_{n,m}(p))=p$.
By double induction on $n,m:\NN$ we get that $f_{n,m}(g_{n,m}(x))=x$
for all $x:\Eq_\NN(n,m)$ 
(all four cases are identifications in a propositional type).

Now it follows by \cref{lem:equivalence-via-inverse} 
that each $f_{n,m}$ is an equivalence.
\end{proof}

\begin{xca}\label{xca:set-sum}
Show that $X\amalg Y$ is a set if $X$ and $Y$ are sets.
\end{xca}

\section{More on natural numbers}
\label{sec:more-on-N}

Define inductively addition on $\NN$,
by $n+0\defeq n$ and $n+S(m)\defeq S(n+m)$. Define
multiplication on $\NN$, setting $m\cdot 0\defeq 0$ and 
$m\cdot(n+1)\defeq (m\cdot n) + m$. 
Another useful function is the \emph{predecessor} $p$ defined by
$p(0)\defeq 0$ and $p(S(n))\defeq n$.
Elementary properties of addition, multiplication and predecessor
can be proved in type theory in the usual way.
We freely use them, sometimes even in definitions, leaving most of the
proofs to the reader.

\begin{definition}
\label{def:orderonN}
Let $n,m:\NN$. We say that $n$ is less than or equal to $m$, and write $n\leq m$,
if there is a $k:\NN$ such that $k+n=m$. This $k$ is unique and we say 
that $n$ is less than $m$, and write $n<m$, if this $k$ is not $0$.
Both $n\leq m$ and $n<m$ are propositions for all $m,n:\NN$.
\end{definition}

\begin{xca}\label{xca:try-your-luck-N}
Try your luck in type theory proving any of the following.
The successor function satisfies $(S(n)=S(m))\liff(n=m)$.
The functions $+$ and $\cdot$ are commutative and associative,
$\cdot$ distributes over $+$.
The relations $\leq$ and $<$ are transitive and
preserved under $+$; $\leq$ also under $\cdot$. 
We have $(m\leq n) \liff ((m<n)\amalg(m=n))$ (so $\leq$ is reflexive).
Furthermore, $((m\leq n)\times (n\leq m)) \liff (m=n)$,
and $((m<n)\times(n<m))\to\false$ (so $<$ is irreflexive).
\end{xca}

We can prove the following lemma by double induction, avoiding the 
Law of the Excluded Middle (LEM).

\begin{lemma}\label{lem:dec-eq+order-N}
The relations $=$, $\leq$ and $<$ on $\NN$ are decidable.
\end{lemma}

We will now prove an important property of $\NN$, called the
\emph{least number principle for decidable, non-empty subsets of} $\NN$.
We give some more details of the proof, since they illustrate an aspect
of type theory that has not been very prominent up to know, namely
the close connection between proving and programming.

\begin{definition}
\label{def:Niswellordered}
Let $P(n)$ be a proposition for all natural numbers $n$. 
Define the type $P_{\min}(n)$ expressing that $n$ is the smallest
natural number such that $P(n)$:
\[
P_{\min}(n) \defeq P(n) \times \prod_{m:\NN} (P(m) \to n\leq m)
\]
Then $P_{\min}(n)$ is also a proposition, and all $n$ such that $P_{\min}(n)$
are equal. Therefore the type $\sum_{n:\NN} P_{\min}(n)$
is also a proposition. 

For any function $d(n): P(n)\amalg\neg P(n)$ deciding $P(n)$ for each $n:\NN$, 
we define a function $\mu_P:\NN\to\NN$ which,
given input $n$, searches for a $k<n$ such that $P(k)$.
If such a $k$ exists, $\mu_P$ returns the least such $k$,
otherwise $\mu_P(n)=n$.
This is a standard procedure that we will call \emph{bounded search}.
The function $\mu_P$ is defined by induction, setting
$\mu_P(0)\defeq 0$ and 
$\mu_P(S(n))\defeq \mu_P(n)$ if $\mu_P(n) < n$.
Otherwise, we set $\mu_P(S(n))\defeq n$ if $P(n)$,
and $\mu_P(S(n))\defeq S(n)$ else, using $d(n)$ to decide,
that is, by induction on $d(n):P(n)\amalg\neg P(n)$.
By design, $\mu_P$ `remembers' where it has found the least $k$.
We are now done with the computational part and the rest
is a correctness proof.

By induction on $n:\NN$ and $d(n): P(n)\amalg\neg P(n)$ we show
\[
%((\mu_P(n)\leq n) \times (\mu_P(n)<n \to P(\mu_P(n)))).
\mu_P(n)\leq n \quad\text{and}\quad \mu_P(n)<n \to P(\mu_P(n)).
\]
The base case $n=0$ is by Ex Falso. For the induction step, 
review the computation of $\mu_P(S(n))$. If $\mu_P(S(n))=\mu_P(n)$
since $\mu_P(n) < n$, then we are done by the induction hypothesis.
Otherwise, either $\mu_P(S(n))=n$ and $P(n)$, or $\mu_P(S(n))=S(n)$.
In both cases we are done.

Also by induction on $n:\NN$ and $d(n): P(n)\amalg\neg P(n)$ we show
\[
P(m) \to \mu_P(n)\leq m,~ \text{for all $m$ in $\NN$.}
\]
The base case $n=0$ holds since $\mu_P(0)=0$. For the induction step,
assume $P(m) \to \mu_P(n)\leq m$ for all $m$. 
We have either $\mu_P(S(n))=\mu_P(n)$, 
or $\mu_P(n)=n$ and $\neg P(n)$ and $\mu_P(S(n)) = S(n)$. 
In both cases we are done by using the induction hypothesis.
From this we get by by contraposition
\[
\mu_P(n)= n \to \neg P(m),~\text{for all $m<n$.}
\]

Note that there may not be an $n$ such that $P(n)$,
the best we can do is to prove
\[
P(n)\to P_{\min}(\mu_P(S(n)))
\]
by combining previous results. Assume $P(n)$.
Then $\mu_P(S(n))\leq n < S(n)$, so that $P(\mu_P(S(n)))$.
Moreover, $P(m) \to \mu_P(S(n))\leq m$ for all $m$ in $\NN$.
Hence $P_{\min}(\mu_P(S(n)))$.

Since $\sum_{n:\NN} P_{\min}(n)$ is a proposition, 
we obtain the following function by the induction
principle for propositional truncation \cref{lem:prop-trunc-elim}:

\begin{align}\label{eqn:min}%\tag{bla}
\min(P) : \prod_{n:\NN}(P(n)\amalg\neg P(n)) \to 
           \Trunc{\sum_{n:\NN} P(n)} \to  \sum_{n:\NN} P_{\min}(n).
\end{align}
\end{definition}

\begin{remark}\label{rem:computations-can-decide}
In the interest of readability, we do not always make the use
of witnesses of decidability in computations explicit.
A typical example is the case distinction on $\mu_P(n) < n$ in
\cref{def:Niswellordered} above. This remark applies to all
sets and decidable relations on them. We shall immediately put
this convention to good use in the proof of a form of the so-called
\emph{Pigeon Hole Principle} (PHP).
\end{remark}

\begin{lemma}\label{lem:PHP}
For all $k:\NN$ and $f:\NN\to\NN$ such that $f(n)<k$
for all $n<k+1$, there exist $n,m < k+1$ such that 
$n\neq m$ and $f(n)=f(m)$.
\end{lemma}
\begin{proof}
By induction on $k$. The base case $k=0$ is by Ex Falso.
For the induction case $k+1$, assume the lemma proved for $k$
(induction hypothesis, IH, for all $f$). Let $f$ be such 
that $f(n)<k+1$ for all $n<k+2$. The idea of the proof is
to search for an $n<k+1$ such that $P(n)\defeq (f(n)=k)$,
by computing $\mu_P(k+1)$ as in \cref{def:Niswellordered}.
If $\mu_P(k+1)=k+1$, that is, $f(n)<k$ for all $n<k+1$,
then we are done by IH. Assume $\mu_P(k+1) < k+1$,
so $f(\mu_P(k+1))=k$.
If also $f(k+1)=k$ then we are done.
If $f(k+1)<k$, then we define $g$ by $g(n)=f(k+1)$
if $f(n)= k$, and $g(n)=f(n)$ otherwise. 
Then IH applies to $g$, and we get $n,m < k+1$ such that 
$n\neq m$ and $g(n)=g(m)$. If $f(n)=f(m)$
we are of course done. Otherwise, $f(n),f(m)$ cannot both
be smaller than $k$, as $g(n)=g(m)$. And we are done in both
remaining cases, $f(n)=g(n)=g(m)=f(k+1)$ and $f(k+1)=g(n)=g(m)=f(m)$.
\end{proof}

We can now rule out the existence of equivalences between finite
sets of different size.
\begin{corollary}\label{cor:Fin-n-injective}
If $n>m$, then $(\sum_{k:\NN} k<n) \neq (\sum_{k:\NN} k<m)$.
\end{corollary}


\footnote{BID: I have stolen the section on integers and put it in the circle chapters as agreed.  The digression below should then be incorporated a bit later when we mature from and start talking about groups more generally.  I think it will fit wonderfully quite early as an example.}

\section{Digression: the integers as a group}
\label{sec:integers-group}

In the previous section we first defined the \emph{set}
of integers $\zet$. Then we defined functions that add
algebraic stucture to $\zet$ (the structure of a free group 
with one generator, see \cref{XX}. In this section we
show that this structure can also be obtained from the
symmetries of a certain element in a certain type.
This is one of the keypoints of this book, 
therefore pointed out early on.

\begin{lemma}\label{lem:one-orbit-int}
Let $s$ be as in \cref{def:integers}, and 
let $f:\zet\to\zet$ be such that $f\circ s = s\circ f$. 
  \begin{enumerate}
  \item\label{item-one-orbit} For all $z:\zet$ there is a unique $n:\NN$
such that either $z=s^{-(n+1)}(0)$, or $z=s^{n}(0)$.
  \item\label{item-f(0)-nonneg} For all $n:\NN$, if $f(0)=s^{n}(0)$, then $f=s^{n}$.
  \item\label{item-f(0)-nonpos} For all $n:\NN$, if $f(0)=s^{-n}(0)$, then $f=s^{-n}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
From $f\circ s = s\circ f$ we get $f\circ s^n = s^n\circ f$
and $f\circ s^{-n} = s^{-n}\circ f$ by induction on $n:\NN$.

(\ref{item-one-orbit}) Induction on $n:\NN$ proves $s^{n}(0)=n$, 
as well as $s^{-n}(0)=-n$. Uniqueness is easy.

(\ref{item-f(0)-nonneg}) Assume $f(0)=s^{n}(0)$.  
Given $z:\zet$, let $m$ be such that either $z=s^{m}(0)$, 
or $z=s^{-(m+1)}(0)$. In the first case we calculate
\[
f(z)=f(s^{m}(0))=s^{m}(f(0))=s^{m}(s^{n}(0))=s^{n}(s^{m}(0))= s^{n}(z),
\]
so that $f=s^{n}$ by function extensionality. 
The second case is very similar;
(\ref{item-f(0)-nonpos}) goes like (\ref{item-f(0)-nonneg}).
\end{proof}

\begin{corollary}\label{cor:pre-torsor-int}
We have $\zet\equiv \sum_{f:\zet\to\zet} (f\circ s = s\circ f)$.
\end{corollary}
\begin{proof}
First observe that $f\circ s = s\circ f$ is a true proposition
for all $f=s^n$ and $f=s^{-n}$. Recall that proofs of propositions
may be left out from dependent pairs. Thus we
define $e : \zet\to \sum_{f:\zet\to\zet} (f\circ s = s\circ f)$ 
inductively by setting 
$e(z_0)\defeq \id_\zet$, 
$e(pos(n))\defeq s^{n+1}$,
$e(neg(n))\defeq s^{-(n+1)}$.
By \cref{lem:one-orbit-int}, $e$ is a well-defined equivalence.
\end{proof}

Again by \cref{lem:one-orbit-int}, if $f\circ s = s\circ f$,
then $f: \zet\to\zet$ is an equivalence. 
Using UA, we get by \cref{cor:pre-torsor-int} the equivalence
\[
\zet\equiv \sum_{f:\zet=\zet} (f\circ s \circ f^{-1} = s).
\]
Recall from \cref{sec:heavy-transport} 
that $f\circ s \circ f^{-1}$ is transport of $s$ by
conjugation with $f$. Using the characterization of equality 
in $\Sigma$-types from \cref{sec:sum-types} we get the equivalence
\[
\zet\equiv ((\zet,s) = (\zet,s)).
\]
One particular equivalence is $e$ from the proof
of \cref{cor:pre-torsor-int}, with inverse $e^{-1}$.
We have $e^{-1}(\id_\zet) = 0$.
The type $(\zet,s) = (\zet,s)$ of symmetries of $(\zet,s)$
has a natural algebraic structure induced by
$\trans$ and $\symm$ from \cref{def:eq-symm}.
%if $f,g:(\zet,s) = (\zet,s)$, then $f\circ g:(\zet,s) = (\zet,s)$.
%Moreover, elements of $(\zet,s) = (\zet,s)$ have inverses.
This algebraic structure is transported by $e^{-1}$ to $\zet$
and gives exactly the group structure defined by ${+},{-},0$.
The proof of this observation is postponed until the notion
of a group has been defined in \cref{ch:groups}.

One important issue has been ignored up to now:
What is the type of $(\zet,s)$? 
One possible answer is: $\sum_{X:\UU}(X=X)$.
The following exercise shows that we do not get the 
property that $(X,f)=(X,f)$ is equivalent to
$(\zet,s) = (\zet,s)$ for all $(X,f) : \sum_{X:\UU}(X=X)$.
It is for this reason that in \cref{sec:gsets} 
another choice will be made.

\begin{xca}\label{xca:zet-symmetries}
Figure out the symmetries of $(\zet,\id_\zet)$ (easy) and 
of $(\zet,s^2)$ (hard).
\end{xca}

\section{The type of finite types}
\label{sec:typeFin}
Recall from \cref{sec:finite-types} the types
$\false$, $\true$ and $\bool$ containing zero, one and two
elements, respectively. We now define generally the
type of $n$ elements for any $n:\NN$.
%$\bn{0} $, $\bn{1} $, $\bn{2} ,\ldots$

\begin{definition}\label{def:finiteset}
For any type $X$ define $\Succ(X)\defeq X\amalg\true$.
Define inductively the type family $F(n)$, for each $n:\NN$, by
setting $F(0)\defeq\bn{0}$ and $F(S(n))\defeq\Succ(F(n))$.
Now abbreviate $F(n)\defeq\bn{n}$. The type $\bn{n}$ is called
the type with $n$ elements, and we denote its elements
by $0,1,\ldots,n-1$ rather than by the corresponding expressions
using $\inl{}$ and $\inr{}$.
%\end{definition}

\begin{xca}\label{xca:finite-types}
\hspace{1in}
  \begin{enumerate}
  \item Denote in full all elements of $\bn{0} $, $\bn{1} $, and $\bn{2}$.
  \item Show (using UA) that $\bn{1} =\true$, $\bn{2} =\bool$.
  \item Show by PHP that $n=m$ if $\bn{m}=\bn{n}$.
  \end{enumerate}
\end{xca}
  
\begin{lemma}\label{lem:maxonefinitetype}
\hspace{1in}%BAD STYLE?
\begin{enumerate}
  \item $\sum_{n:\NN}\Trunc{X=\bn{n}}$ is a proposition, for all types $X$.
  \item 
$\sum_{X:\UU}\sum_{n:\NN}\Trunc{X=\bn{n}} =\sum_{X:\UU}\Trunc{\sum_{n:\NN}X=\bn{n}}.$
  \end{enumerate}
\end{lemma}
\begin{proof}

(1) Assume $(n,p),(m,q): \sum_{n:\NN}\Trunc{X=\bn{n}}$.
Then we have $\Trunc{\bn{n}=\bn{m}}$, so $\Trunc{n=m}$
by \cref{xca:finite-types}. But $\NN$ is a set by \cref{thm:isset-inductive-types}, 
so $\Trunc{n=m}=(n=m)$. It follows that $(n,p)=(m,q)$.

(2) Follows from $\sum_{n:\NN}\Trunc{X=\bn{n}} =\Trunc{\sum_{n:\NN}X=\bn{n}}$,
which is easily proved by giving functions in both directions and using UA.
\end{proof}

The above lemma remains of course true if $X$ ranges over $\Set$.
If a set $S$ is in the same component in $\Set$ as $\bn{n}$ we say that \emph{$S$ has cardinality $n$} or that \emph{the cardinality of $S$ is $n$}.

\begin{definition}\label{def:groupoidFin}
The \emph{groupoid of finite sets} is defined by
\[
\fin\defeq\sum_{S:\Set}\Trunc{\sum_{n:\NN}S=\bn{n}}.
\]
For $n:\NN$, the \emph{groupoid of sets of cardinality $n$} is defined by
\[
\fin_n\defequi \sum_{S:\Set}\Trunc{S=\bn{n}}.
\]
\end{definition}
Observe that $\fin_1=\bn{1} $ and $\fin=\sum_{n:\NN}\fin_n$
by \cref{lem:maxonefinitetype}.

\section{Subtypes and Decidability}
\label{sec:subtype}

In ordinary sets, an a subset $A$ of a set $B$ can be given concretely by 
considering the \emph{characteristic function} $\chi_A\colon B\to\bool$ 
which takes the value $\yes$ on $A$ and $\no$ everywhere else: 
$A$ is then exactly the preimage of $\yes$ under $\chi_A: B\to\bool$.

These considerations translate to types, though there are some 
subtleties. Let $T$ be a type and consider a function $f:T\to\bool$
and a type family $P: T\to\UU$. One difference is that $\bool$ is 
a set consisting of two elements, and $P(t)$ is a type.
The difference becomes smaller when we assume $P(t)$ to be a
proposition for all $t:T$. This leads to the definition of the 
\emph{subtype of $T$ given by $P$} as the sum type $T_P \defeq \sum_{t:T} P(t)$.

Given $f:T\to\bool$ we can now form the 
subtype $T_f \defeq \sum_{t:T} (f(t) = \yes)$.
The subtype $T_f$ is special: we can prove by
double induction on $x,y:\bool$ that $(x=y)\amalg\neg(x=y)$
(even easier than for $\NN$). Hence we have
$(f(t) = \yes)\amalg(f(t) = \no)$ for all $t$ in $T$.
That is, we can decide for every $t$ in $T$ whether $t$ is in the
subtype defined by $f$ or not. Therefore we call $T_f$ the
\emph{decidable subtype of $T$ given by $f$}. 


in that we have not assumed that any proposition must be equal to either of the sets $\bn 0$ and $\bn 1$.  
\begin{definition}
  \label{def:subtype}
  Let $B$ be a type and $P(b)$ a family of propositions depending on $b:B$.  Then the first projection $\sum_{b:B}P(b)\to B$ is said to be the \emph{inclusion of the subtype $\sum_{b:B}P(b)$ in $B$}.\index{subtype}\footnote{what do you prefer; injection or inclusion?  They don't mean the same thing and personally I'd prefer injection here.}  

We say that an element $b':B$ \emph{lies in the subtype} $\sum_{b:B}P(b)$ if $P(b')$.
\end{definition}

Alternatively, a function $f:A\to B$ in \emph{injective} if all the preimages $f^{-1}(b)$ are propositions.

\begin{definition}
  \label{def:decidableprop}
  A \emph{decidable proposition}\index{decidable proposition} depending on a type $A$ is a function $P:A\to\bn 2$.  The underlying proposition is then given by considering $0:\bn 2$ as the empty type $\bn 0$ and $1:\bn 2$ as the singleton $\bn 1$.  A set $A$ is \emph{decidable}\index{decidable set} (or \emph{has decidable equality}\index{decidable equality}) if the propositions $a=_Aa'$ are decidable.  A subtype $\sum_{a:A}P(a)$ is \emph{decidable}\index{decidable subtype} if the propositions $P(a)$ are decidable. 
\end{definition}


% Local Variables:
% fill-column: 144
% latex-block-names: ("lemma" "theorem" "remark" "definition" "corollary" "fact" "properties" "conjecture" "proof" "question" "proposition")
% TeX-master: "book"
% End:
